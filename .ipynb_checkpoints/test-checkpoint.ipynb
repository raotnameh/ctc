{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import json, pickle, os, string, tqdm, kenlm, json\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import groupby\n",
    "import Levenshtein as Lev\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lse(*args): # logsumexp trick\n",
    "    a_max = max(args)\n",
    "    lsp = math.log(sum(math.exp(a-a_max) for a in args))\n",
    "    return a_max + lsp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s1 = True text\n",
    "#s2 = predicted text\n",
    "\n",
    "def wer_(s1, s2):\n",
    "    \"\"\"\n",
    "    Computes the Word Error Rate, defined as the edit distance between the\n",
    "    two provided sentences after tokenizing to words.\n",
    "    Arguments:\n",
    "        s1 (string): space-separated sentence\n",
    "        s2 (string): space-separated sentence\n",
    "    \"\"\"\n",
    "\n",
    "    # build mapping of words to integers\n",
    "    b = set(s1.split() + s2.split())\n",
    "    word2char = dict(zip(b, range(len(b))))\n",
    "\n",
    "    # map the words to a char array (Levenshtein packages only accepts\n",
    "    # strings)\n",
    "    w1 = [chr(word2char[w]) for w in s1.split()]\n",
    "    w2 = [chr(word2char[w]) for w in s2.split()]\n",
    "    \n",
    "    return Lev.distance(''.join(w1), ''.join(w2))\n",
    "\n",
    "def cer_(s1, s2):\n",
    "    \"\"\"\n",
    "    Computes the Character Error Rate, defined as the edit distance.\n",
    "\n",
    "    Arguments:\n",
    "        s1 (string): space-separated sentence\n",
    "        s2 (string): space-separated sentence\n",
    "    \"\"\"\n",
    "    s1, s2, = s1.replace(' ', ''), s2.replace(' ', '')\n",
    "\n",
    "    return Lev.distance(s1, s2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#When using the above implementation, use the code belove to calculate the wer in percentatge: \n",
    "#pred = list of ouput prediction of model (it is the text) # example [\" MY NAME IS HEMANT\", \" I AM A GOD\"]\n",
    "# total_wer = 0\n",
    "# for x in range(len(pred)):\n",
    "#     transcript, reference = data_[x][1], pred[x]\n",
    "#     wer_inst = wer(transcript, reference)\n",
    "#     total_wer += float(wer_inst)\n",
    "# print(\"WER is : \",total_wer/len(pred),\"%\")\n",
    "# wer_(pred,true)/len(pred.split(' '))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = np.load(\"out.npy\")\n",
    "with open(\"true.txt\", \"r\") as f:\n",
    "    reference = f.read()\n",
    "with open(\"pred.txt\", \"r\") as f:\n",
    "    transcript = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_best_path(out,labels):\n",
    "    \"implements best path decoding as shown by Graves\"\n",
    "    out = [labels[i] for i in np.argmax(out, axis=1) if i!=labels[-1]]\n",
    "    o = \"\"\n",
    "    for i,j in groupby(out):\n",
    "        o = o + i\n",
    "    return o.replace(\"_\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = \"_'ABCDEFGHIJKLMNOPQRSTUVWXYZ \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUR PRODATIVITY AND OUR COMMITMENT TO OUR CLIENTS XXXUM TO MAKE SURE THAT WE NEED WHAT BE XXXUM PROMISED FOR THE DEADLINE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19.047619047619047"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gred_txt = ctc_best_path(out,labels)\n",
    "print(gred_txt)\n",
    "wer_(gred_txt,reference)/len(reference.split(' '))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WORD LM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_w = kenlm.LanguageModel('/home/hemant/sopi_deep/lm/3_gram_full.arpa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_beam(ptot,k):\n",
    "    if len(ptot) < k:\n",
    "        return [i for i in ptot.keys()]\n",
    "    else:\n",
    "        dict_ = sorted(dict((v,k) for k,v in ptot.items()).items(),reverse=True)[:k]\n",
    "        return [i[1] for i in dict_]\n",
    "\n",
    "#using WORD LM\n",
    "def ctc_beam_search(out,labels, prune=0.0001, k=20, lm=None,alpha=0.3,beta=12):\n",
    "    \"implements CTC Prefix Search Decoding Algo13.043478260869565%'rithm as shown by Graves\"\n",
    "    '''\n",
    "    out = ctc output\n",
    "    labels = string of labels\n",
    "    prune = prune the ctc output\n",
    "    k=beam-width\n",
    "    lm=word age model used\n",
    "    alpha,beta = hyper-parameters\n",
    "    '''\n",
    "\n",
    "    bc_i = 0 # blank/special charatcter index \n",
    "    F = out.shape[1]\n",
    "    out = np.vstack((np.zeros(F), out))\n",
    "    steps = out.shape[0]\n",
    "    \n",
    "    pb, pnb = defaultdict(Counter), defaultdict(Counter)\n",
    "    pb[0][''], pnb[0][''] = 1, 0\n",
    "    prev_beams = ['']\n",
    "    for t in range(1,steps):\n",
    "        pruned_alphabet = [labels[i] for i in np.where(out[t] > prune)[0]]\n",
    "        for b in prev_beams:\n",
    "            \n",
    "            for c_t in pruned_alphabet:\n",
    "                index = labels.index(c_t)\n",
    "                #Collapsing case (copy case as the last character in the beam)\n",
    "                if c_t == \"_\": #Extending with a blank\n",
    "                    pb[t][b] = out[t][index]*(pb[t-1][b] + pnb[t-1][b])\n",
    "                    continue\n",
    "                else:\n",
    "                    i_plus = b + c_t\n",
    "                    if len(b) > 0 and c_t == b[-1]: #Extending with the same character as the last one\n",
    "                        pnb[t][b] += out[t][index]*pnb[t-1][b]\n",
    "                        pnb[t][i_plus] += out[t][index]*pb[t-1][b]\n",
    "                    #Extending case as the last character is different\n",
    "                    elif c_t == \" \" and len(b.replace(' ', '')) > 0 : # LM constraints\n",
    "                        prob = [i[0] for i in lm.full_scores(i_plus,eos=False,bos=False)][-1]\n",
    "                        lm_p = (10**prob)**alpha\n",
    "                        pnb[t][i_plus] += lm_p*out[t][index]*(pb[t-1][b] + pnb[t-1][b])*(len(b.split())+1)**beta\n",
    "                    else:\n",
    "                        pnb[t][i_plus] += out[t][index]*(pb[t-1][b] + pnb[t-1][b])\n",
    "                    # If the new beam is not in the previous beams\n",
    "                    if i_plus not in prev_beams:\n",
    "                        pb[t][i_plus] += out[t][labels.index(\"_\")] * (pb[t - 1][i_plus] + pnb[t - 1][i_plus])\n",
    "                        pnb[t][i_plus] += out[t][index] * pnb[t - 1][i_plus]\n",
    "\n",
    "        ptot = pb[t] + pnb[t]\n",
    "        prev_beams = sort_beam(ptot,k)\n",
    "    return prev_beams[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-222-76bbaea655ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbeam_txt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctc_beam_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlm_w\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.96\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeam_txt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mwer_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeam_txt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreference\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreference\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-221-43418f8191c9>\u001b[0m in \u001b[0;36mctc_beam_search\u001b[0;34m(out, labels, prune, k, lm, alpha, beta)\u001b[0m\n\u001b[1;32m     43\u001b[0m                     \u001b[0;31m#Extending case as the last character is different\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                     \u001b[0;32melif\u001b[0m \u001b[0mc_t\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;31m# LM constraints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                         \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_plus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m                         \u001b[0mlm_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                         \u001b[0mpnb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi_plus\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlm_p\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpnb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "beam_txt = ctc_beam_search(out,labels,0.0001,k=100,lm=lm_w,alpha=1.96,beta=6)\n",
    "print(beam_txt)\n",
    "wer_(beam_txt,reference)/len(reference.split(' '))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import collections\n",
    "\n",
    "NEG_INF = -float(\"inf\")\n",
    "\n",
    "def make_new_beam():\n",
    "  fn = lambda : (NEG_INF, NEG_INF)\n",
    "  return collections.defaultdict(fn)\n",
    "\n",
    "def logsumexp(*args):\n",
    "  \"\"\"\n",
    "  Stable log sum exp.\n",
    "  \"\"\"\n",
    "  if all(a == NEG_INF for a in args):\n",
    "      return NEG_INF\n",
    "  a_max = max(args)\n",
    "  lsp = math.log(sum(math.exp(a - a_max)\n",
    "                      for a in args))\n",
    "  return a_max + lsp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length(sequence):\n",
    "    for i in h.items():\n",
    "        m = i[0]\n",
    "    g = ''\n",
    "    for i in m:\n",
    "        g+=str(i)\n",
    "\n",
    "def decode(probs, beam_size=2, blank=0, lm=None,alpha=0.3,beta=0):\n",
    "    T, S = probs.shape\n",
    "    probs = np.log(probs)\n",
    "    beam = [(tuple(), (0.0, NEG_INF))]\n",
    "\n",
    "    for t in range(T): # Loop over time\n",
    "        next_beam = make_new_beam()\n",
    "\n",
    "        for prefix, (p_b, p_nb) in beam: # Loop over beam\n",
    "            \n",
    "            for s in range(S): # Loop over vocab\n",
    "                p = probs[t, s]\n",
    "                if s == blank:\n",
    "                    n_p_b, n_p_nb = next_beam[prefix]\n",
    "                    n_p_b = logsumexp(n_p_b, p_b + p, p_nb + p)\n",
    "                    next_beam[prefix] = (n_p_b, n_p_nb)\n",
    "                    continue\n",
    "                \n",
    "                end_t = prefix[-1] if prefix else None\n",
    "                n_prefix = prefix + (s,)\n",
    "                a =''\n",
    "                for i in n_prefix:\n",
    "                    a += labels[i]\n",
    "                \n",
    "                n_p_b, n_p_nb = next_beam[n_prefix]\n",
    "                \n",
    "                if s != end_t:\n",
    "                    n_p_nb = logsumexp(n_p_nb, p_b + p, p_nb + p)\n",
    "                elif len(a.replace(' ', '')) > 0:\n",
    "                    prob = [i[0] for i in lm.full_scores(a,eos=False,bos=False)][-1]\n",
    "                    n_p_nb = logsumexp(n_p_nb + p, p_b + p)\n",
    "                \n",
    "                next_beam[n_prefix] = (n_p_b, n_p_nb)\n",
    "                if s == end_t:\n",
    "                    n_p_b, n_p_nb = next_beam[prefix]\n",
    "                    n_p_nb = logsumexp(n_p_nb + p, p_nb + p)\n",
    "                    next_beam[prefix] = (n_p_b, n_p_nb)\n",
    "        beam = sorted(next_beam.items(),\n",
    "                key=lambda x : (logsumexp(*x[1])),\n",
    "                reverse=True)\n",
    "        beam = beam[:beam_size]\n",
    "\n",
    "    best = beam[0]\n",
    "    return best[0], -logsumexp(*best[1]),next_beam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 9.169\n",
      "OUR PRODATIVITY AND OUR COMMITMENT TO OUR CLIENTS XXXUM TO MAKE SURE THAT WE NEED WHAT BE XXXUM PROMISED FOR THE DEADLINE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19.047619047619047"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label, score, h = decode(out, beam_size=2, blank=0, lm=lm_w,alpha=1,beta=4)\n",
    "print(\"Score {:.3f}\".format(score))\n",
    "a =''\n",
    "for i in label:\n",
    "    a += labels[i]\n",
    "print(a)\n",
    "wer_(a,reference)/len(reference.split(' '))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
