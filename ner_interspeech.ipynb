{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from time import time\n",
    "import json, pickle, os, string, kenlm, json\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import groupby\n",
    "import Levenshtein as Lev\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lse(*args):\n",
    "  \"\"\"\n",
    "  Stable log sum exp.\n",
    "  \"\"\"\n",
    "  if all(a == -float('inf') for a in args):\n",
    "      return -float('inf')\n",
    "  a_max = max(args)\n",
    "  lsp = math.log(sum(math.exp(a - a_max)\n",
    "                      for a in args))\n",
    "  return a_max + lsp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s1 = True text\n",
    "#s2 = predicted text\n",
    "\n",
    "def wer_(s1, s2):\n",
    "    \"\"\"\n",
    "    Computes the Word Error Rate, defined as the edit distance between the\n",
    "    two provided sentences after tokenizing to words.\n",
    "    Arguments:\n",
    "        s1 (string): space-separated sentence\n",
    "        s2 (string): space-separated sentence\n",
    "    \"\"\"\n",
    "\n",
    "    # build mapping of words to integers\n",
    "    b = set(s1.split() + s2.split())\n",
    "    word2char = dict(zip(b, range(len(b))))\n",
    "\n",
    "    # map the words to a char array (Levenshtein packages only accepts\n",
    "    # strings)\n",
    "    w1 = [chr(word2char[w]) for w in s1.split()]\n",
    "    w2 = [chr(word2char[w]) for w in s2.split()]\n",
    "    \n",
    "    return Lev.distance(''.join(w1), ''.join(w2))\n",
    "\n",
    "def cer_(s1, s2):\n",
    "    \"\"\"\n",
    "    Computes the Character Error Rate, defined as the edit distance.\n",
    "\n",
    "    Arguments:\n",
    "        s1 (string): space-separated sentence\n",
    "        s2 (string): space-separated sentence\n",
    "    \"\"\"\n",
    "    s1, s2, = s1.replace(' ', ''), s2.replace(' ', '')\n",
    "\n",
    "    return Lev.distance(s1, s2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#When using the above implementation, use the code belove to calculate the wer in percentatge: \n",
    "#pred = list of ouput prediction of model (it is the text) # example [\" MY NAME IS HEMANT\", \" I AM A GOD\"]\n",
    "# total_wer = 0\n",
    "# for x in range(len(pred)):\n",
    "#     transcript, reference = data_[x][1], pred[x]\n",
    "#     wer_inst = wer(transcript, reference)\n",
    "#     total_wer += float(wer_inst)\n",
    "# print(\"WER is : \",total_wer/len(pred),\"%\")\n",
    "# wer_(pred,true)/len(pred.split(' '))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = np.load(\"out.npy\")[0]\n",
    "with open(\"true.txt\", \"r\") as f:\n",
    "    reference = f.read().strip()\n",
    "    \n",
    "with open(\"pred.txt\", \"r\") as f:\n",
    "    transcript = f.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_best_path(out,labels):\n",
    "    \"implements best path decoding as shown by Graves\"\n",
    "    out = [labels[i] for i in np.argmax(out, axis=1) if i!=labels[-1]]\n",
    "    o = \"\"\n",
    "    for i,j in groupby(out):\n",
    "        o = o + i\n",
    "    return o.replace(\"_\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = \"_'ABCDEFGHIJKLMNOPQRSTUVWXYZ \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND CHARGED IFEVER HE MIGHT FIND SIR GAWANE AND SIR UWANE TO BRING THEM TO THE COURT AGAIN AND THEN WERE THEY ALL GLAD AND SO PRAY DHAS OR MORE HOUSE TO RIDE WITH THEM TO THE KING'S COURT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23.077"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gred_txt = ctc_best_path(out,labels)\n",
    "print(gred_txt)\n",
    "np.round(wer_(gred_txt,reference)/len(reference.split(' '))*100,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WORD LM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_w = kenlm.LanguageModel('/home/hemant/asr_wm/data/ner/align_wav_txt/4_gram.arpa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_beam(ptot,k):\n",
    "    if len(ptot) < k:\n",
    "        return [i for i in ptot.keys()]\n",
    "    else:\n",
    "        dict_ = sorted(dict((v,k) for k,v in ptot.items()).items(),reverse=True)[:k]\n",
    "        return [i[1] for i in dict_]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_beam_search(out,labels, prune=0.0001, k=20, lm=None,alpha=0.3,beta=12):\n",
    "    \n",
    "    bc_i = 0 # blank/special charatcter index \n",
    "    F = out.shape[1]\n",
    "    dummy_ = np.vstack((np.zeros(F), out))\n",
    "    out = np.log(np.vstack((np.zeros(F), out)))\n",
    "    steps = out.shape[0]\n",
    "    \n",
    "    pb, pnb = defaultdict(Counter), defaultdict(Counter)\n",
    "    pb[0][''], pnb[0][''] = 0, -float(\"inf\")\n",
    "    prev_beams = ['']\n",
    "    for t in tqdm(range(1,steps)):\n",
    "        pruned_alphabet = [labels[i] for i in np.where(dummy_[t] > prune)[0]]\n",
    "        for b in prev_beams:\n",
    "            \n",
    "            for c_t in pruned_alphabet:\n",
    "                index = labels.index(c_t)\n",
    "                if c_t == \"_\": #Extending with a blank\n",
    "                    pb[t][b] = lse(pb[t][b],out[t][index]+pb[t-1][b], out[t][index]+pnb[t-1][b])\n",
    "                    \n",
    "                    continue\n",
    "                \n",
    "                else:\n",
    "                    i_plus = b + c_t\n",
    "                    if len(b.replace(' ', '')) > 0  and c_t == b[-1]: #Extending with the same character as the last one\n",
    "                        pnb[t][b] = lse(pnb[t][b],out[t][index]+pnb[t-1][b])\n",
    "                        pnb[t][i_plus] = lse(pnb[t][i_plus],out[t][index]+pb[t-1][b])\n",
    "                    elif c_t == \" \" and len(b.replace(' ', '')) > 0 : # LM constraints\n",
    "                        words = b.strip()\n",
    "                        prob = 10**[i for i in lm.full_scores(words,eos=False,bos=False)][-1][0]\n",
    "                        prob = alpha*lse(math.log(prob))\n",
    "                        word_inser = len(words.split())+1\n",
    "                        word_inser = beta*lse(math.log(word_inser))\n",
    "                        score_ = prob + word_inser\n",
    "                        pnb[t][i_plus] = lse(out[t][index]+pb[t-1][b],out[t][index] + pnb[t-1][b]) + score_\n",
    "                        \n",
    "                        print(words,pnb[t][i_plus],',',prob,word_inser,score_)\n",
    "#                         pnb[t][i_plus] = pnb[t][i_plus] + score_\n",
    "                    else:\n",
    "                        pnb[t][i_plus] = lse(pnb[t][i_plus],out[t][index]+pb[t-1][b], out[t][index]+pnb[t-1][b])\n",
    "\n",
    "#                     If the new beam is not in the previous beams\n",
    "                    if i_plus not in prev_beams:\n",
    "                        pb[t][i_plus] = lse(pb[t][i_plus],out[t][labels.index(\"_\")]+pb[t - 1][i_plus], out[t][labels.index(\"_\")]+ pnb[t - 1][i_plus])\n",
    "                        pnb[t][i_plus] = lse(pnb[t][i_plus],out[t][index] + pnb[t - 1][i_plus])\n",
    "\n",
    "        ptot = pb[t] + pnb[t]\n",
    "        prev_beams = sort_beam(ptot,k)\n",
    "#         print('\\n')\n",
    "        \n",
    "    return prev_beams[0], pb, pnb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hemant/.conda/envs/deep/lib/python3.6/site-packages/ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46132bb78c8d462dbf821826768c0182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=681.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "beam_txt, pb, pnb = ctc_beam_search(out[:],labels,0.000,k=100,lm=lm_w,alpha=0.58,beta=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAND  CHARGED  IFEVER HE  MIGHT FIND SIR GAWAINE AND SIR U WAIYINE TO BRING THEM TO THE COAURT A GAIN  AND THEN I WERE THEY ALL GLAD D  AND SO I A PRAIYE DE I S S OR MO RE HO USE S TO RI DE WITH HE M TO THE KING'S COOURT E \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "74.359"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(beam_txt)\n",
    "np.round(wer_(beam_txt,reference)/len(reference.split(' '))*100,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54, 39)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(beam_txt.split()), len(reference.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"AND CHARGED IF EVER HE MIGHT FIND SIR GAWAINE AND SIR UWAINE TO BRING THEM TO THE COURT AGAIN AND THEN WERE THEY ALL GLAD AND SO PRAYED THEY SIR MARHAUS TO RIDE WITH THEM TO THE KING'S COURT\""
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/hemant/E2E_NER-Through-Speech/S2T/\")\n",
    "from opts import add_decoder_args, add_inference_args\n",
    "from utils import load_model\n",
    "import os\n",
    "# from ctc_decoders import *\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from data.data_loader import SpectrogramParser\n",
    "\n",
    "from opts import add_decoder_args, add_inference_args\n",
    "from utils import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "decoding = 'beam_w'\n",
    "prune = 0.00001\n",
    "beam_width = 50\n",
    "alpha = 1\n",
    "beta = 4\n",
    "lm = kenlm.LanguageModel('/home/hemant/asr_wm/data/ner/align_wav_txt/4_gram.arpa')\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "device = torch.device(\"cuda\")\n",
    "model = load_model(device, \"/home/hemant/asr_wm/models/deep/final.pth\")\n",
    "spect_parser = SpectrogramParser(model.audio_conf, normalize=True)\n",
    "\n",
    "torch.cuda.set_device(int(0))\n",
    "with open(\"/home/hemant/asr_wm/dev.csv\",\"r\") as f:\n",
    "    csv = f.readlines()\n",
    "\n",
    "total_cer, total_wer, num_tokens, num_chars = 0, 0, 0, 0\n",
    "lm = lm_w\n",
    "output = []\n",
    "from tqdm.auto import tqdm\n",
    "for i in tqdm(csv[:1]):\n",
    "    audio_path, reference_path = i.split(\",\")\n",
    "\n",
    "    spect = spect_parser.parse_audio(audio_path).contiguous()\n",
    "    spect = spect.view(1, 1, spect.size(0), spect.size(1))\n",
    "    spect = spect.to(device)\n",
    "\n",
    "    input_sizes = torch.IntTensor([spect.size(3)]).int()\n",
    "    out, output_sizes = model(spect, input_sizes)\n",
    "    out = out.cpu().detach().numpy()[0]\n",
    "\n",
    "    transcript = ctc_beam_search(out,labels,prune,beam_width,lm,alpha,beta)\n",
    "#     transcript = ctc_best_path(out,labels)\n",
    "    with open(reference_path.replace(\"\\n\",\"\"),\"r\") as f:\n",
    "        reference = f.readline()\n",
    "        \n",
    "#     break\n",
    "    output.append([transcript,reference])\n",
    "    wer_inst = wer_(transcript,reference)\n",
    "    cer_inst = cer_(transcript, reference)\n",
    "    total_wer += wer_inst\n",
    "    total_cer += cer_inst\n",
    "    num_tokens += len(reference.split(' '))\n",
    "    num_chars += len(reference.replace(' ', ''))\n",
    "\n",
    "        \n",
    "wer = (float(total_wer) / num_tokens)*100\n",
    "cer = (float(total_cer) / num_chars)*100\n",
    "print('Test Summary \\t'\n",
    "    'Average WER {wer:.3f}\\t'\n",
    "    'Average CER {cer:.3f}\\t?'.format(wer=wer, cer=cer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha, beta = np.linspace(0.2,2,15), np.linspace(2,10,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha, beta = np.linspace(0.2,2,15), np.linspace(0,6,15)\n",
    "values = [[i,j] for j in beta for i in alpha]\n",
    "\n",
    "total_cer, total_wer, num_tokens, num_chars = 0, 0, 0, 0\n",
    "lm = lm_w\n",
    "output = []\n",
    "\n",
    "for alpha,beta in tqdm(values):\n",
    "    for i in csv:\n",
    "        audio_path, reference_path = i.split(\",\")\n",
    "\n",
    "        spect = spect_parser.parse_audio(audio_path).contiguous()\n",
    "        spect = spect.view(1, 1, spect.size(0), spect.size(1))\n",
    "        spect = spect.to(device)\n",
    "\n",
    "        input_sizes = torch.IntTensor([spect.size(3)]).int()\n",
    "        out, output_sizes = model(spect, input_sizes)\n",
    "        out = out.cpu().detach().numpy()[0]\n",
    "\n",
    "        if decoding == \"greedy\": transcript = ctc_best_path(out,labels)\n",
    "        elif decoding == \"beam_w\": transcript = ctc_beam_search(out,labels,prune,beam_width,lm,alpha,beta)\n",
    "        elif decoding == \"beam_c\": transcript = ctc_beam_search_clm(out,labels,prune,beam_width,lm,alpha=alpha,beta=beta)\n",
    "\n",
    "        with open(reference_path.replace(\"\\n\",\"\"),\"r\") as f:\n",
    "            reference = f.readline()\n",
    "        wer_inst = wer_(transcript,reference)\n",
    "        cer_inst = cer_(transcript, reference)\n",
    "        total_wer += wer_inst\n",
    "        total_cer += cer_inst\n",
    "        num_tokens += len(reference.split(' '))\n",
    "        num_chars += len(reference.replace(' ', ''))\n",
    "    np.save(\"/home/hemant/ctc_decoders/abw.npy\",np.array(output))\n",
    "    wer = (float(total_wer) / num_tokens)*100\n",
    "    cer = (float(total_cer) / num_chars)*100\n",
    "    output.append([alpha,beta,wer])\n",
    "    print(\"aplha: \",alpha,\"beta: \",beta)\n",
    "    print('Test Summary \\t'\n",
    "        'Average WER {wer:.3f}\\t'\n",
    "        'Average CER {cer:.3f}\\t'.format(wer=wer, cer=cer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
