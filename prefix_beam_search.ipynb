{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import json, pickle, os, string, tqdm, kenlm, json\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import groupby\n",
    "import Levenshtein as Lev\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s1 = True text\n",
    "#s2 = predicted text\n",
    "\n",
    "def wer_(s1, s2):\n",
    "    \"\"\"\n",
    "    Computes the Word Error Rate, defined as the edit distance between the\n",
    "    two provided sentences after tokenizing to words.\n",
    "    Arguments:\n",
    "        s1 (string): space-separated sentence\n",
    "        s2 (string): space-separated sentence\n",
    "    \"\"\"\n",
    "\n",
    "    # build mapping of words to integers\n",
    "    b = set(s1.split() + s2.split())\n",
    "    word2char = dict(zip(b, range(len(b))))\n",
    "\n",
    "    # map the words to a char array (Levenshtein packages only accepts\n",
    "    # strings)\n",
    "    w1 = [chr(word2char[w]) for w in s1.split()]\n",
    "    w2 = [chr(word2char[w]) for w in s2.split()]\n",
    "    \n",
    "    return Lev.distance(''.join(w1), ''.join(w2))\n",
    "\n",
    "def cer_(s1, s2):\n",
    "    \"\"\"\n",
    "    Computes the Character Error Rate, defined as the edit distance.\n",
    "\n",
    "    Arguments:\n",
    "        s1 (string): space-separated sentence\n",
    "        s2 (string): space-separated sentence\n",
    "    \"\"\"\n",
    "    s1, s2, = s1.replace(' ', ''), s2.replace(' ', '')\n",
    "\n",
    "    return Lev.distance(s1, s2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#When using the above implementation, use the code belove to calculate the wer in percentatge: \n",
    "#pred = list of ouput prediction of model (it is the text) # example [\" MY NAME IS HEMANT\", \" I AM A GOD\"]\n",
    "# total_wer = 0\n",
    "# for x in range(len(pred)):\n",
    "#     transcript, reference = data_[x][1], pred[x]\n",
    "#     wer_inst = wer(transcript, reference)\n",
    "#     total_wer += float(wer_inst)\n",
    "# print(\"WER is : \",total_wer/len(pred),\"%\")\n",
    "# wer_(pred,true)/len(pred.split(' '))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_best_path(out,labels):\n",
    "    \"implements best path decoding as shown by Graves\"\n",
    "    out = [labels[i] for i in np.argmax(out, axis=1) if i!=labels[-1]]\n",
    "    o = \"\"\n",
    "    for i,j in groupby(out):\n",
    "        o = o + i\n",
    "    return o.replace(\"_\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-ecbcb9045f7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgred_txt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctc_best_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgred_txt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mwer_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgred_txt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreference\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgred_txt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'out' is not defined"
     ]
    }
   ],
   "source": [
    "gred_txt = ctc_best_path(out,labels)\n",
    "print(gred_txt)\n",
    "wer_(gred_txt,reference)/len(gred_txt.split(' '))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WORD LM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lse(*args):\n",
    "    a_max = max(args)\n",
    "    lsp = math.log(sum(math.exp(a-a_max) for a in args))\n",
    "    return a_max + lsp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4076059644443806"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lse(1,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.40760596444438"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.log(sum(math.exp(a) for a in [1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_w = kenlm.LanguageModel('/home/hemant/sopi_deep/lm/3_gram_full.arpa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_beam(ptot,k):\n",
    "    if len(ptot) < k:\n",
    "        return [i for i in ptot.keys()]\n",
    "    else:\n",
    "        dict_ = sorted(dict((v,k) for k,v in ptot.items()).items(),reverse=True)[:k]\n",
    "\n",
    "        return [i[1] for i in dict_]\n",
    "\n",
    "#using WORD LM\n",
    "def ctc_beam_search(out,labels, prune=0.0001, k=20, lm=None,alpha=0,beta=0):\n",
    "    \"implements CTC Prefix Search Decoding Algorithm as shown by Graves\"\n",
    "    '''\n",
    "    out = ctc output\n",
    "    labels = string of labels\n",
    "    prune = prune the ctc output\n",
    "    k=beam-width\n",
    "    lm=word age model used\n",
    "    alpha,beta = hyper-parameters\n",
    "    '''\n",
    "\n",
    "    bc_i = 0 # blank/special charatcter index \n",
    "    F = out.shape[1]\n",
    "    out = np.vstack((np.zeros(F), out))\n",
    "    steps = out.shape[0]\n",
    "\n",
    "    pb, pnb = defaultdict(Counter), defaultdict(Counter)\n",
    "    pb[0][''], pnb[0][''] = 1, 0\n",
    "    prev_beams = ['']\n",
    "    for t in range(1,steps):\n",
    "        pruned_alphabet = [labels[i] for i in np.where(out[t] > prune)[0]]\n",
    "        for b in prev_beams:\n",
    "            for c_t in pruned_alphabet:\n",
    "                index = labels.index(c_t)\n",
    "                #Collapsing case (copy case as the last character in the beam)\n",
    "                if c_t == \"_\": #Extending with a blank\n",
    "                    pb[t][b] += out[t][index]*(pb[t-1][b] + pnb[t-1][b])\n",
    "                    continue\n",
    "                else:\n",
    "                    i_plus = b + c_t\n",
    "                    if len(b) > 0 and c_t == b[-1]: #Extending with the same character as the last one\n",
    "                        pnb[t][b] += out[t][index]*pnb[t-1][b]\n",
    "                        pnb[t][i_plus] += out[t][index]*pb[t-1][b]\n",
    "                    #expanding the beam (extend case as the last character is different)\n",
    "                    elif c_t == \" \" and len(b.replace(' ', '')) > 0 : # LM constraints\n",
    "                        prob = [i[0] for i in lm.full_scores(' '.join(i_plus.split(\" \")[-4:-1]),eos=False,bos=False)][-1]\n",
    "                        lm_p = (10**prob)**alpha\n",
    "                        pnb[t][i_plus] += lm_p*out[t][index]*(pb[t-1][b] + pnb[t-1][b])\n",
    "                    else:\n",
    "                        pnb[t][i_plus] += out[t][index]*(pb[t-1][b] + pnb[t-1][b])\n",
    "\n",
    "                    if i_plus not in prev_beams:\n",
    "                        pb[t][i_plus] += out[t][index] * (pb[t - 1][i_plus] + pnb[t - 1][i_plus])\n",
    "                        pnb[t][i_plus] += out[t][index] * pnb[t - 1][i_plus]\n",
    "        \n",
    "        ptot = pb[t] + pnb[t]\n",
    "        for i in ptot.keys():\n",
    "            ptot[i] = (ptot[i] * (len(i.split(' '))+1))**beta\n",
    "        prev_beams= sort_beam(ptot,k)\n",
    "    return prev_beams[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I THINK IS IS GOINGNA BE VORY ALD IDEA IN THE BEAST IN INTELLETION ALDATI BUSINESSES IN THE YEARS TO COME I DONTWT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "52.17391304347826"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beam_txt=ctc_beam_search(out,labels,0.00001,k=100,lm=lm_w,alpha=0,beta=6)\n",
    "print(beam_txt)\n",
    "wer_(beam_txt,reference)/len(beam_txt.split(' '))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I THINK ITS ITS GONNA BE VERY GOOD IDEA IN THE BIS IN THE INTERNATIONAL BISHNES BUSINESSES IN THE YEARS TO COME XXXAH I DONT HAVE'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHARACTER LM Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_c = kenlm.LanguageModel('/home/hemant/junk/3_gram.arpa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_beam(ptot,k):\n",
    "    if len(ptot) < k:\n",
    "        return [i for i in ptot.keys()]\n",
    "    else:\n",
    "        dict_ = sorted(dict((v,k) for k,v in ptot.items()).items(),reverse=True)[:k]\n",
    "        return [i[1] for i in dict_]\n",
    "\n",
    "#using CHARACTER LM\n",
    "def ctc_beam_search_clm(out,labels, prune=0.001, k=20, lm_w = None, lm_c=None,alpha=0,beta=0):\n",
    "    \"implements CTC Prefix Search Decoding Algorithm as shown by Graves\"\n",
    "    \n",
    "    '''\n",
    "    out = ctc output\n",
    "    labels = string of labels\n",
    "    prune = prune the ctc output\n",
    "    k=beam-width\n",
    "    lm=charac language model used\n",
    "    alpha,beta = hyper-parameters\n",
    "    '''\n",
    "    \n",
    "    bc_i = 0 # special charatcter index \n",
    "    F = out.shape[1]\n",
    "    out = np.vstack((np.zeros(F), out))\n",
    "    steps = out.shape[0]\n",
    "    \n",
    "    pb, pnb = defaultdict(Counter), defaultdict(Counter)\n",
    "    pb[0][''], pnb[0][''] = 1, 0\n",
    "    prev_beams = ['']\n",
    "    for t in range(1,steps):\n",
    "        pruned_alphabet = [labels[i] for i in np.where(out[t] > prune)[0]]\n",
    "        for b in prev_beams:\n",
    "            for c_t in pruned_alphabet:\n",
    "                index = labels.index(c_t)\n",
    "                #Collapsing case (copy case as the last character in the beam)\n",
    "                if c_t == \"_\": #Extending with a blank\n",
    "                    pb[t][b] += out[t][index]*(pb[t-1][b] + pnb[t-1][b])  \n",
    "                else:  # LM constraints\n",
    "                    i_plus = b + c_t\n",
    "                     #Extending with the same character as the last one\n",
    "                    if len(b) > 0 and c_t == b[-1]:\n",
    "                        pnb[t][b] += out[t][index]*pnb[t-1][b]\n",
    "                        pnb[t][i_plus] += out[t][index]*pb[t-1][b]\n",
    "                        \n",
    "                    #expanding the beam (extend case as the last character is different)\n",
    "                    elif len(b.replace(' ', '')) > 0 :                       \n",
    "#                         if c_t == \" \" and lm_w != None: # word LM constraints\n",
    "#                             if lm_w != None:\n",
    "#                                 prob = [i[0] for i in lm_w.full_scores(' '.join(i_plus.split(\" \")[-4:-1]),eos=False,bos=False)][-1]\n",
    "#                                 lm_p = (10**prob)**alpha\n",
    "#                             else: lm_p =1\n",
    "#                             pnb[t][i_plus] += lm_p*out[t][index]*(pb[t-1][b] + pnb[t-1][b])\n",
    "#                         else: # character LM constraint\n",
    "                        if lm_c != None:\n",
    "                            prob = [i[0] for i in lm_c.full_scores(i_plus,eos=False,bos=False)][-1]\n",
    "                            lm_p = (10**prob)**alpha\n",
    "                        else: lm_p = 1\n",
    "                        pnb[t][i_plus] += lm_p*out[t][index]*(pb[t-1][b] + pnb[t-1][b])\n",
    "                    \n",
    "                    else:\n",
    "                        pnb[t][i_plus] += out[t][index]*(pb[t-1][b] + pnb[t-1][b])\n",
    "                        \n",
    "                    if i_plus not in prev_beams:\n",
    "                        pb[t][i_plus] += out[t][index] * (pb[t - 1][i_plus] + pnb[t - 1][i_plus])\n",
    "                        pnb[t][i_plus] += out[t][index] * pnb[t - 1][i_plus]\n",
    "                        \n",
    "        ptot = pb[t] + pnb[t]\n",
    "        for i in ptot.keys():\n",
    "            ptot[i] = ptot[i]*(len(i.split())+1)**beta\n",
    "        prev_beams = sort_beam(ptot,k)\n",
    "    return prev_beams[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPLEMANTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/hemant/E2E_NER-Through-Speech/S2T/\")\n",
    "from opts import add_decoder_args, add_inference_args\n",
    "from utils import load_model\n",
    "import os\n",
    "from ctc_decoders import *\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from data.data_loader import SpectrogramParser\n",
    "\n",
    "from opts import add_decoder_args, add_inference_args\n",
    "from utils import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = \"_'ABCDEFGHIJKLMNOPQRSTUVWXYZ \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 1/50 [00:00<00:17,  2.86it/s]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 2/50 [00:00<00:16,  2.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 3/50 [00:01<00:17,  2.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 4/50 [00:01<00:18,  2.54it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 5/50 [00:02<00:18,  2.37it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 6/50 [00:02<00:14,  2.94it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 7/50 [00:02<00:14,  3.02it/s]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 8/50 [00:02<00:14,  2.90it/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 9/50 [00:03<00:13,  2.97it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|██        | 10/50 [00:03<00:12,  3.10it/s]\u001b[A\u001b[A\n",
      "\n",
      " 22%|██▏       | 11/50 [00:03<00:12,  3.01it/s]\u001b[A\u001b[A\n",
      "\n",
      " 24%|██▍       | 12/50 [00:04<00:13,  2.88it/s]\u001b[A\u001b[A\n",
      "\n",
      " 26%|██▌       | 13/50 [00:04<00:12,  3.00it/s]\u001b[A\u001b[A\n",
      "\n",
      " 28%|██▊       | 14/50 [00:04<00:11,  3.18it/s]\u001b[A\u001b[A\n",
      "\n",
      " 30%|███       | 15/50 [00:05<00:11,  3.14it/s]\u001b[A\u001b[A\n",
      "\n",
      " 32%|███▏      | 16/50 [00:05<00:11,  3.04it/s]\u001b[A\u001b[A\n",
      "\n",
      " 34%|███▍      | 17/50 [00:05<00:11,  2.95it/s]\u001b[A\u001b[A\n",
      "\n",
      " 36%|███▌      | 18/50 [00:06<00:11,  2.80it/s]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███▊      | 19/50 [00:06<00:10,  2.92it/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████      | 20/50 [00:06<00:10,  2.78it/s]\u001b[A\u001b[A\n",
      "\n",
      " 42%|████▏     | 21/50 [00:07<00:09,  2.93it/s]\u001b[A\u001b[A\n",
      "\n",
      " 44%|████▍     | 22/50 [00:07<00:10,  2.64it/s]\u001b[A\u001b[A\n",
      "\n",
      " 46%|████▌     | 23/50 [00:08<00:09,  2.72it/s]\u001b[A\u001b[A\n",
      "\n",
      " 48%|████▊     | 24/50 [00:08<00:09,  2.66it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 25/50 [00:08<00:09,  2.70it/s]\u001b[A\u001b[A\n",
      "\n",
      " 52%|█████▏    | 26/50 [00:09<00:09,  2.64it/s]\u001b[A\u001b[A\n",
      "\n",
      " 54%|█████▍    | 27/50 [00:09<00:09,  2.45it/s]\u001b[A\u001b[A\n",
      "\n",
      " 56%|█████▌    | 28/50 [00:10<00:09,  2.31it/s]\u001b[A\u001b[A\n",
      "\n",
      " 58%|█████▊    | 29/50 [00:10<00:08,  2.39it/s]\u001b[A\u001b[A\n",
      "\n",
      " 60%|██████    | 30/50 [00:10<00:08,  2.50it/s]\u001b[A\u001b[A\n",
      "\n",
      " 62%|██████▏   | 31/50 [00:11<00:07,  2.39it/s]\u001b[A\u001b[A\n",
      "\n",
      " 64%|██████▍   | 32/50 [00:11<00:07,  2.32it/s]\u001b[A\u001b[A\n",
      "\n",
      " 66%|██████▌   | 33/50 [00:12<00:07,  2.42it/s]\u001b[A\u001b[A\n",
      "\n",
      " 68%|██████▊   | 34/50 [00:12<00:06,  2.59it/s]\u001b[A\u001b[A\n",
      "\n",
      " 70%|███████   | 35/50 [00:13<00:06,  2.38it/s]\u001b[A\u001b[A\n",
      "\n",
      " 72%|███████▏  | 36/50 [00:13<00:06,  2.09it/s]\u001b[A\u001b[A\n",
      "\n",
      " 74%|███████▍  | 37/50 [00:14<00:05,  2.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 76%|███████▌  | 38/50 [00:14<00:05,  2.31it/s]\u001b[A\u001b[A\n",
      "\n",
      " 78%|███████▊  | 39/50 [00:14<00:04,  2.30it/s]\u001b[A\u001b[A\n",
      "\n",
      " 80%|████████  | 40/50 [00:15<00:04,  2.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 82%|████████▏ | 41/50 [00:15<00:04,  1.96it/s]\u001b[A\u001b[A\n",
      "\n",
      " 84%|████████▍ | 42/50 [00:16<00:03,  2.34it/s]\u001b[A\u001b[A\n",
      "\n",
      " 86%|████████▌ | 43/50 [00:16<00:02,  2.38it/s]\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████▊ | 44/50 [00:17<00:02,  2.40it/s]\u001b[A\u001b[A\n",
      "\n",
      " 90%|█████████ | 45/50 [00:17<00:02,  2.39it/s]\u001b[A\u001b[A\n",
      "\n",
      " 92%|█████████▏| 46/50 [00:17<00:01,  2.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 94%|█████████▍| 47/50 [00:18<00:01,  2.43it/s]\u001b[A\u001b[A\n",
      "\n",
      " 96%|█████████▌| 48/50 [00:18<00:00,  2.48it/s]\u001b[A\u001b[A\n",
      "\n",
      " 98%|█████████▊| 49/50 [00:18<00:00,  2.56it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 50/50 [00:19<00:00,  2.57it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Summary \tAverage WER 99.764\tAverage CER 25.093\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# decoding = 'greedy'\n",
    "decoding = 'beam_w'\n",
    "# decoding = 'beam_c'\n",
    "prune = 0.00001\n",
    "beam_width = 10\n",
    "alpha = 10\n",
    "beta = 24\n",
    "lm = lm_w\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "device = torch.device(\"cuda\")\n",
    "model = load_model(device, \"/home/hemant/sopi_deep/models/deep/finetuen_sopi.pth\")\n",
    "spect_parser = SpectrogramParser(model.audio_conf, normalize=True)\n",
    "\n",
    "torch.cuda.set_device(int(0))\n",
    "with open(\"/home/hemant/sopi_deep/data/deepspeech/test/ata/test.csv\",\"r\") as f:\n",
    "    csv = f.readlines()\n",
    "\n",
    "total_cer, total_wer, num_tokens, num_chars = 0, 0, 0, 0\n",
    "lm = lm_w\n",
    "output = []\n",
    "for i in tqdm(csv):\n",
    "    audio_path, reference_path = i.split(\",\")\n",
    "\n",
    "    spect = spect_parser.parse_audio(audio_path).contiguous()\n",
    "    spect = spect.view(1, 1, spect.size(0), spect.size(1))\n",
    "    spect = spect.to(device)\n",
    "\n",
    "    input_sizes = torch.IntTensor([spect.size(3)]).int()\n",
    "    out, output_sizes = model(spect, input_sizes)\n",
    "    out = out.cpu().detach().numpy()[0]\n",
    "\n",
    "    if decoding == \"greedy\": transcript = ctc_best_path(out,labels)\n",
    "    elif decoding == \"beam_w\": transcript = ctc_beam_search(out,labels,prune,beam_width,lm,alpha,beta)\n",
    "    elif decoding == \"beam_c\": transcript = ctc_beam_search_clm(out,labels,prune,beam_width,lm,alpha=alpha,beta=beta)\n",
    "        \n",
    "    with open(reference_path.replace(\"\\n\",\"\"),\"r\") as f:\n",
    "        reference = f.readline()\n",
    "    output.append([transcript,reference])\n",
    "    wer_inst = wer_(transcript,reference)\n",
    "    cer_inst = cer_(transcript, reference)\n",
    "    total_wer += wer_inst\n",
    "    total_cer += cer_inst\n",
    "    num_tokens += len(reference.split())\n",
    "    num_chars += len(reference.replace(' ', ''))\n",
    "        \n",
    "wer = (float(total_wer) / num_tokens)*100\n",
    "cer = (float(total_cer) / num_chars)*100\n",
    "print('Test Summary \\t'\n",
    "    'Average WER {wer:.3f}\\t'\n",
    "    'Average CER {cer:.3f}\\t'.format(wer=wer, cer=cer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' OURPRODATIVITYANDOURCOMMITMENTTOOURACLIENTSXXXUMTOMAKESURETHATWENEEDWHATBEXXXUMPROMISED FORTHE DEADLINE', 'OUR PRODUCTIVITY AND OUR COMMITMENT TO OUR CLIENTS XXXUM TO MAKE SURE WHAT WE NEED WHAT WE PROMISED FOR THE DEADLINE'] \n",
      "\n",
      "['    MEALMYHISIMPORTANTXXXUMIIWOULDTELLMSURMURPHYTHATXXXUMTHOUHWEWOULDPAYBU', 'SO IT REALLY IS IMPORTANT I WOULD TELL MR MURPHY THAT THOUGH WE WOULD PAY'] \n",
      "\n",
      "[' HIMISWILSONTHISISRUNXXXUMYMACHLYGONBECAUSEXXXUHIMSTILLANTHEIRFORTHEFLIGHTSANDDELAYESINCETHEREISBADSTORM', 'HI MISS WILSON THIS IS RYAN XXXUM IM ACTUALLY CALLING BECAUSE XXXUH IM STILL IN AIRPORT THE FLIGHTS BEEN DELAYED SINCE THERE IS A BAD STORM'] \n",
      "\n",
      "[' SOIIUCANNOTICANATTRAINTAMEETINGIMSORRYIIRIGHIREALLYPOTRIIESFORWIRTHISSOMESSAGE', 'SO I CANNOT I CANNOT JOIN THE MEETING I XXXST I REALLY APOLOGISE FOR THIS MESSAGE'] \n",
      "\n",
      "['  YOUKNOWITSNOTWHATIAHADPLANNEDXHYOHICASXXHIEJUSTTEVERIKEEPYOUNOTD', 'BAD BUT YOU KNOW ITS NOT WHAT I HAD PLANNED SO YEAH I GUESS ILL I JUST BE FOR KEEP YOU NOTIFIED'] \n",
      "\n",
      "['  THEYAREONTHANKYOU', 'LATER ON THANK YOU'] \n",
      "\n",
      "[' MYPROFESSIONALGOALSANDPLANESFORTHENEXTDENYEARSISIMLOOKINGFORTHESUCCESSO', 'MY PROFESSIONAL GOALS AND PLANS FOR THE NEXT TEN YEARS IS IM LOOKING FOR THE SUCCESS OF'] \n",
      "\n",
      "['  ISHCOMPANYANDIMLOKARDIMIMLOOKINGFORWARFORTHEPROMOTIONANDISEEY', 'BRITISH COMPANY AND IM LOOKING IM IM LOOKING FORWARD FOR THE PROMOTION AND I SEE MY'] \n",
      "\n",
      "[' SELFWHORKINGINTHEREDISHCOMPANIGSUCCESSFUL', 'SELF WORKING IN THE BRITISH COMPANY SUCCESSFUL'] \n",
      "\n",
      "['  IHOPEMYACAREERWHILLEBESECCESSFULAFTHERKEENYEARSFRONOW', 'I HOPE MY CAREER WILL BE SUCCESSFUL AFTER TEN YEARS FROM NOW'] \n",
      "\n",
      "['  CRINCSIMICAANUNSENDAYGAREATTHECOMPANYHEADQUARTERSBECAUSE', 'HEY CHRIS I AM AGAINST AN ONSITE DAYCARE AT THE COMPANY HEADQUARTERS BECAUSE'] \n",
      "\n",
      "[' ITWILLDESTERMEWHELEWAREKINGANDITTOLITWILLRERTRYOUAREWORKINGAND', 'IT WILL DISTURB ME WHILE WORKING AND IT WILL DISTURB YOU WHILE WORKING AND'] \n",
      "\n",
      "[' YHANDCHILDRENNIISVERYANICCEANDTHEYWILLDISHER', 'YEAH AND CHILDREN IS VERY NICEY AND THEY WILL DISTURB'] \n",
      "\n",
      "['ANDITWILLIRICATEMANYEMPLOYEESBECAUSE  CHILDRENNIS', 'AND IT WILL IRRITATE MANY EMPLOYEES BECAUSE CHILDREN IS'] \n",
      "\n",
      "[' TALKINGALODANDPLAINGTHEYWILLBEPLAYATTHELOYT', 'TALKING LOUD AND PLAYING THEY WILL BE PLAY AT DE LAVI'] \n",
      "\n",
      "[' ANDYEAHIAMAGAINSOMASIDETAKECAREATTHE  COMPANYSHEADQUARTERSBECAUSEITCAES', 'AND YEAH I AM AGAINST A ONSITE DAYCARE AT THE COMPANY HEADQUARTERS BECAUSE IT WAS'] \n",
      "\n",
      "['  WILLDISHERBUSITWILLDISTARVYDTHEMPLOEESITWILL DISTERMTHENEAROFFICESAND', 'IT WILL DISTURB US IT WILL DISTURB THE EMPLOYEES IT WILL DISTURB THE NEAR OFFICES AND'] \n",
      "\n",
      "[' ANTHEMAMIESWHERENOTWILLLETMEFORCUSSONTHEREWERE', 'AND THE XXXUN WILL NOT BE FOCUS ON THEIR WORK'] \n",
      "\n",
      "[' ANDANDYEAAHTHEYSHOULDNOVPV', 'AND AND YEAH THEY SHOULD NOT PUT ON'] \n",
      "\n",
      "['DIGGARSOTHEREISXATTENITHELCOMPANYSHEARDQUURTERSSOTHAT', 'DAYCARE CENTERS AT THE AT THE COMPANYS HEADQUARTERS SO THAT'] \n",
      "\n",
      "['   THATHATYOURFELUSTHOGETNIMPLAESMAKEACUSALITERAWORK', 'SO THAT IT WILL FOCUS SO THAT THE EMPLOYEES MAY FOCUS ON THEIR WORK'] \n",
      "\n",
      "['   SIRWEHAVETOSHAWISTICKNPLEMPTATIONOORBIDDINGOUREMPLOYESFROMACCESSINGSOLDIANMEDIAWEBSITESINENWILEATWORKBECAUSE', 'YES SIR WE HAVE TO HAVE A STRICT OF FORBIDDING OUR EMPLOYEES FROM ACCESSING SOCIAL MEDIA WEBSITES WHILE WHILE XXXST WHILE AT WORK BECAUSE'] \n",
      "\n",
      "[' THEYRE  HEREINOURCOMPANYTOWORKFOCUSINTHEIRWORKANDHAVEA  CONTENTAATIONONTHEWORKONCEWELALLOWTHEMTO', 'THEY ARE HERE IN OUR COMPANY TO WORK FOCUS ON THEIR WORK AND TO HAVE A CONCENTRATION ON THEIR WORK ONCE WE ALLOW THEM TO'] \n",
      "\n",
      "[' ACCESSUCIALLNEJIAAWEBSITEWHILEONWORKITWILLDISTRACTTHEMFROMTHEIRWORKANDWILLFOCUSADANDWILLFOCUSAND', 'ACCESS SOCIAL MEDIA WEBSITE WHILE IN WORK IT WILL DISTRACT THEM FROM THEIR WORK AND WILL FOCUS AND AND WILL FOCUS AND'] \n",
      "\n",
      "['   THEYWILLHAVEAHALFOCUSONTHEWORKINSOMETIMESIFWELALLOWEDTHEMFROMDOINGTHATMOSTOFTHETIMEBASEDONE', 'AND AND THEY WILL HAVE A HALF FOCUS ON THEIR WORK AND SOMETIMES IF WE ALLOW THEM FROM DOING THAT MOST OF THE TIME BASED ON'] \n",
      "\n",
      "[' CORDBASEANDSERVICSSOMEOFOUREMVLOYESSOMEOFTEMPLOYESCANNOTXXXUHCANNOTACOMPLETEANDCANNOTFINISH', 'RECORDS BASED ON SURVEYS SOME OF OUR EMPLOYEES SOME OF THE EMPLOYEES CANNOT XXXUH CANNOT COMPLETE AND CANNOT FINISH'] \n",
      "\n",
      "['  JOBDANONTIMEBECAUSEOFTHEDESTRUCTIONOFSOCIALMEDIAREMSITESSOWHILETITDISVERYADIBELIEVEITJISTAVERYAVERYIMPORTANTTHATWEHAVETOFORS', 'THE JOB DONE ON TIME BECAUSE OF THE DISTRACTION OF SOCIAL MEDIA WEBSITES SO WHILE IT IS VERY AND I BELIEVE IT IS VERY VERY IMPORTANT THAT WE HAVE TO FORBID'] \n",
      "\n",
      "['  EMPLOYEESFROMACCESSINGSARCECIALLNEWJOEBSITEWHILLEWHILEATWORKBECAUSENOTNOTINTWENTYFORHOURSTHATTHEYAREAWORKINGTHEYCANHAVETEISOCIALNEDI', 'OUR EMPLOYEES FROM ACCESSING SOCIAL MEDIA WEBSITES WHILE WHILE AT WORK BECAUSE NOT NOT INTERMITTENTLY FOR HOURS THEY ARE WORKING THEY CAN HAVE THEIR SOCIAL MEDIA'] \n",
      "\n",
      "[' WEBSITEXXHACCESSDURINGAFTERWORKORBEFOREWORKANDTHATSAVERYANDITANDITTHEYHAVESOMUCHTIME OF', 'XXXUH WEBSITE XXXUH ACCESS DURING AFTER WORK OR BEFORE WORK AND THATS VERY AND AND XXXST AND THEY HAVE SO MUCH TIME'] \n",
      "\n",
      "['  THEYAREAHEREBECAUSEOFTHEWORKTHATWASGIVENTOTHEMTHEYWEREAHIREDTOFOLLOWTOFOCUSONTHEJOBTHATWASGIVENTOTHEMANDI  GUESS', 'XXXUH THEY ARE HERE BECAUSE OF THE WORK THAT WAS GIVEN TO THEM THEY WERE HIRED TO FOLLOW TO FOCUS ON THE JOB THAT WAS GIVEN TO THEM AND I GUESS'] \n",
      "\n",
      "[' ITISBUTPROPERANFAIRETOAVERYONEIFWOUFORBIDOUREMPLOYESFROMACCESSINGESOCIALYMEEDMEDIAREBSITEWHILEIATWARKTHATWOLLBEALLADTHANKYOU', 'IT IS BUT PROPER AND FAIR TO EVERYONE IF WE FORBID OUR EMPLOYEES FROM ACCESSING SOCIAL MEDIA MEDIA WEBSITE WHILE AT WORK THAT WOULD BE ALL AND THANK YOU'] \n",
      "\n",
      "['    CRINTOREALIZEARMORMINGSIDEDIDNTICODNTITALLYOU', 'TAKE TIME TO REALIZE XXXUN DIDNT I DIDNT I TELL YOU'] \n",
      "\n",
      "['  CANTORYOULIFEOIMONYOURSITEDIDINGTEYEUCINTLIKTELLYOUBUTIK', 'TAKE TIME TO REALIZE OH IM ON YOUR SIDE DIDNT I DIDNT I TELL YOU BUT I'] \n",
      "\n",
      "[' ICANXSFELDLITOUTFORYOUYOUKNOWWITSNEVERGONNATEATHATASIMPLE', 'BUT I CAN SPELL IT OUT FOR YOU YOU KNOW ITS NEVER GONNA BE THAT SIMPLE'] \n",
      "\n",
      "['   CANTSPARITOUTFORYOUSOITYOULJUSTREALISEFORICINREALIFEANTHETREXFORAGADHER', 'OH I CAN SPELL IT OUT FOR YOU SO DID YOU JUST REALISE WHAT I JUST REALISE ITLL BE PERFECT FOR EACH OTHER'] \n",
      "\n",
      "['    EVEPEFINEANOTHERICEALIMUNLATUEREALITONEVERWONDERITWINDISTONEONWEACJOTHENNOW', 'AND WILL NEVER FIND ANOTHER THESE FEELINGS WHAT I JUST REALISED DO YOU EVER WONDER IF WE MISSED OUT ON EACH OTHER NO'] \n",
      "\n",
      "[' PLAYCINETREALISEAMONONYOURSHALDIDNTICODINTITELLYOU', 'TAKE TIME TO REALISE I AM ON YOUR SIDE DIDNT I DIDNT I TELL YOU'] \n",
      "\n",
      "[' HOWNOTARUNOTHEPLASSTANDINGNOUTATIONBUTYOUPOULDANQUITEASHOW', 'HOW ABOUT AROUND OF APPLAUSE STANDING OVATION BUT YOU PUT ON QUITE A SHOW'] \n",
      "\n",
      "['    MEETHOINGNOWITTIMETOGOPEKINSANLYCLOSINGTHATWASQUITEASSHOWVELYIN', 'HAD ME GOING NOW ITS TIME TO GO CURTAINS FINALLY CLOSING THAT WAS QUITE A SHOW VERY'] \n",
      "\n",
      "['    PLAININGBUTITSOVERNOW  GOONANDTRATEABBUTITSOVERNOW', 'SUSTAINING BUT ITS OVER NOW GO ON AND TAKE A BOW BUT ITS OVER NOW'] \n",
      "\n",
      "[' GOONANKAYABOUWENISOVERNOWGOONANDTAKEABOT', 'GO ON AND TAKE A BOW BUT ITS OVER NOW GO AND TAKE A BOW'] \n",
      "\n",
      "[' BUTITSOEOGOONEANDKATEAVAW', 'BUT ITS OVER NOW GO ON AND TAKE A BOW'] \n",
      "\n",
      "['  NEVERXMBESSYXXXHIDOWORKATATHOMEIDOATLEASTTHRESETSEXERCISEOWFIRSTISBUSHP', 'WHENEVER IM BUSY XXXUH I DO WORKOUT AT HOME I DO ATLEAST THREE SETS XXXUH EXERCISE FIRST IS PUSH UP'] \n",
      "\n",
      "[' SECONDONEISCARLOVANDTHENTETARITHISJUMKINGJECKXXUHITWILLNOTCONSUMEALOTOFYOURTIMESINCETHOSEAREALL', 'SECOND ONE IS XXXUH CURL UP AND THEN THE THIRD IS JUMPING JACK XXXUH IT WILL NOT CONSUME ALOT OF YOUR TIME SINCE THOSE ARE ALL'] \n",
      "\n",
      "['  THOSEAREONLYSIMPLEXERSASESTHATYOU CANDOATONWITHOUTHAVINGANYGYMQRIPMENT', 'THOSE ARE ONLY SIMPLE EXERCISES THAT YOU CAN DO AT HOME WITHOUT HAVING ANY GYM EQUIPMENT'] \n",
      "\n",
      "[' NOTHINGTHATSOLFTOXXXUHMEINTINAHEALHTIULAPSTELEBEENHOREONTHETITESXUHERESCHEDULE', 'I THINK THATS ALL TO XXXUH MAINTAIN A HEALTHY LIFESTYLE EVEN YOU ARE ON A TIGHT XXXUH SCHEDULE'] \n",
      "\n",
      "['BATSAHBUTSADTEANIDINISGONNABEAGREATARAYYOPPORTUNI', 'XXXUN'] \n",
      "\n",
      "['SHEAPBECAUSESHOLLEARANOTHERLANGUAEASSIAENGLISHTHATSWILLOPENEVERY', 'CHEAP BECAUSE YOU LEARN ANOTHER LANGUAGE ASIDE ENGLISH THATS WILL OPENS EVERY'] \n",
      "\n",
      "['   INEVERYCOMPANYORCOUNTRIWITHTHEENGLISHWITHENINGLESLANGUIGEXXXUM', 'OR IN EVERY COMPANY OR COUNTRY WITH THE ENGLISH WITH ENGLISH LANGUAGE XXXUM'] \n",
      "\n",
      "['    THINKISISGOINGNABEAVORYCALDAIDEAINTHEBEASTINTINTELLETIONALDATIBUSINESSESINTHERYEARSTOCOMEIDONTWT', 'I THINK ITS ITS GONNA BE VERY GOOD IDEA IN THE BIS IN THE INTERNATIONAL BISHNES BUSINESSES IN THE YEARS TO COME XXXAH I DONT HAVE'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in output:\n",
    "    print(i,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    THINKISISGOINGNABEAVORYCALDAIDEAINTHEBEASTINTINTELLETIONALDATIBUSINESSESINTHERYEARSTOCOMEIDONTWT'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I THINK ITS ITS GONNA BE VERY GOOD IDEA IN THE BIS IN THE INTERNATIONAL BISHNES BUSINESSES IN THE YEARS TO COME XXXAH I DONT HAVE'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [9,33,46,50,31,50,0,37,14,0,53,28,20,0,18,46,0,100,25,30,71,20,4,0,3,21,3,15,22,3,0,27,35,6,56,95,46,50,35,20,33,0,25,19,18,50,1100,8,0,30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(a)/50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = audio_path + \",\" +reference_path\n",
    "with open(\"/home/hemant/junk/out.csv\",\"w\") as f:\n",
    "    f.write(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_(transcript,reference)/len(transcript.split(' '))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gred_txt = ctc_best_path(out,labels)\n",
    "print(gred_txt)\n",
    "wer_(gred_txt,reference)/len(gred_txt.split(' '))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_txt=ctc_beam_search(out,labels,0.0001,k=100,lm=lm_w,alpha=0.3,beta=12)\n",
    "print(beam_txt)\n",
    "wer_(beam_txt,reference)/len(beam_txt.split(' '))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_txt=ctc_beam_search_clm(out,labels,0,k=10, lm_c = lm_c, alpha=0.1,beta=6)\n",
    "print(beam_txt)\n",
    "wer_(beam_txt,reference)/len(beam_txt.split(' '))*100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
