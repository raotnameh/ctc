{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json, pickle, os, string, tqdm, kenlm\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import groupby\n",
    "import Levenshtein as Lev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = kenlm.LanguageModel('/home/hemant/deep/lm/libri_lm/3-gram.binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_txt = 'AT FIRST THIS SORT OF THING IS UNPLEASANT ENOUGH IT TOUCHES ONE\\'S SENSE OF HONOUR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wer(s1, s2):\n",
    "    \"\"\"\n",
    "    Computes the Word Error Rate, defined as the edit distance between the\n",
    "    two provided sentences after tokenizing to words.\n",
    "    Arguments:\n",
    "        s1 (string): space-separated sentence\n",
    "        s2 (string): space-separated sentence\n",
    "    \"\"\"\n",
    "\n",
    "    # build mapping of words to integers\n",
    "    b = set(s1.split() + s2.split())\n",
    "    word2char = dict(zip(b, range(len(b))))\n",
    "\n",
    "    # map the words to a char array (Levenshtein packages only accepts\n",
    "    # strings)\n",
    "    w1 = [chr(word2char[w]) for w in s1.split()]\n",
    "    w2 = [chr(word2char[w]) for w in s2.split()]\n",
    "\n",
    "    return str((Lev.distance(''.join(w1), ''.join(w2))/len(s2))*100) + '%'\n",
    "\n",
    "def cer(s1, s2):\n",
    "    \"\"\"\n",
    "    Computes the Character Error Rate, defined as the edit distance.\n",
    "\n",
    "    Arguments:\n",
    "        s1 (string): space-separated sentence\n",
    "        s2 (string): space-separated sentence\n",
    "    \"\"\"\n",
    "    s1, s2, = s1.replace(' ', ''), s2.replace(' ', '')\n",
    "    return str((Lev.distance(s1, s2)/len(s2))*100) + '%'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"out.txt\",\"rb\") as f:\n",
    "    out = pickle.load(f)[0]\n",
    "with open(\"labels.json\") as label_file:\n",
    "    labels = str(''.join(json.load(label_file))) # \"_'ABCDEFGHIJKLMNOPQRSTUVWXYZ \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_best_path(out,labels):\n",
    "    \"implements best path decoding as shown by Graves\"\n",
    "    out = [labels[i] for i in np.argmax(out, axis=1) if i!=labels[-1]]\n",
    "    o = \"\"\n",
    "    for i,j in groupby(out):\n",
    "        o = o + i\n",
    "    return o.replace(\"_\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "gred_txt = ctc_best_path(out,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_beam(ptot,k):\n",
    "    if len(ptot) < k:\n",
    "        return [i for i in ptot.keys()]\n",
    "    else:\n",
    "        dict_ = sorted(dict((v,k) for k,v in ptot.items()).items(),reverse=True)[:k]\n",
    "        return [i[1] for i in dict_]\n",
    "\n",
    "#using WORD LM\n",
    "def ctc_beam_search(out,labels, prune=0.0001, k=20, lm=None,alpha=0.3,beta=12):\n",
    "    \"implements CTC Prefix Search Decoding Algorithm as shown by Graves\"\n",
    "\n",
    "    bc_i = 0 # blank/special charatcter index \n",
    "    F = out.shape[1]\n",
    "    out = np.vstack((np.zeros(F), out))\n",
    "    steps = out.shape[0]\n",
    "    W = lambda l: re.findall(r'\\w+[\\s|>]', l)\n",
    "    \n",
    "    pb, pnb = defaultdict(Counter), defaultdict(Counter)\n",
    "    pb[0][''], pnb[0][''] = 1, 0\n",
    "    prev_beams = ['']\n",
    "    for t in range(1,steps):\n",
    "        pruned_alphabet = [labels[i] for i in np.where(out[t] > prune)[0]]\n",
    "        for b in prev_beams:\n",
    "            for c_t in pruned_alphabet:\n",
    "                index = labels.index(c_t)\n",
    "                #Collapsing case (copy case as the last character in the beam)\n",
    "                if c_t == \"_\": #Extending with a blank\n",
    "                    pb[t][b] += out[t][index]*(pb[t-1][b] + pnb[t-1][b])   \n",
    "                else:\n",
    "                    i_plus = b + c_t\n",
    "                    if len(b) > 0 and c_t == b[-1]: #Extending with the same character as the last one\n",
    "                        pnb[t][b] += out[t][index]*pnb[t-1][b]\n",
    "                        pnb[t][i_plus] += out[t][index]*pb[t-1][b]\n",
    "                    #expanding the beam (extend case as the last character is different)\n",
    "                    elif c_t == \" \" and len(b.replace(' ', '')) > 0 : # LM constraints\n",
    "                        prob = [i[0] for i in lm.full_scores(i_plus,eos=False,bos=False)][-1]\n",
    "                        lm_p = (10**prob)**alpha\n",
    "                        pnb[t][i_plus] += lm_p*out[t][index]*(pb[t-1][b] + pnb[t-1][b])\n",
    "                    else:\n",
    "                        pnb[t][i_plus] += out[t][index]*(pb[t-1][b] + pnb[t-1][b])\n",
    "\n",
    "        ptot = pb[t] + pnb[t]\n",
    "        for i in ptot.keys():\n",
    "            ptot[i] = ptot[i]*(len(i)+1)**beta\n",
    "        prev_beams = sort_beam(ptot,k)\n",
    "    return prev_beams[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "beam_txt=ctc_beam_search(out,labels,0.001,k=1000,lm=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comapring output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true----- AT FIRST THIS SORT OF THING IS UNPLEASANT ENOUGH IT TOUCHES ONE'S SENSE OF HONOUR\n",
      "gred----- AND AT FIRST THIS SORT OF THING IS UNPLEASANT ENOUGH TIT TOUCH ES ONE SENSE OF HONOR\n",
      "beam----- AND AT FIRST THIS SORT OF THING IS UNPLEASANT ENOUGH DHIT TOUCHIS ONE SENSE OF HONOR\n"
     ]
    }
   ],
   "source": [
    "print('true-----',true_txt)\n",
    "print('gred-----',gred_txt)\n",
    "print('beam-----',beam_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_out = 'AND AT FIRST THIS SORT OF THING IS UNPLEASANT ENOUGH DETACHES ONE SENSE OF HONOR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "gred_out = 'AND AT FIRST THIS SORT OF THING IS UNPLEASANT ENOUGH TIT TOUCH ES ONE SENSE OF HONOR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('5.952380952380952%', '13.043478260869565%')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wer(true_txt,beam_txt),cer(true_txt,beam_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_beam(ptot,k):\n",
    "    if len(ptot) < k:\n",
    "        return [i for i in ptot.keys()]\n",
    "    else:\n",
    "        dict_ = sorted(dict((v,k) for k,v in ptot.items()).items(),reverse=True)[:k]\n",
    "        return [i[1] for i in dict_]\n",
    "\n",
    "#using CHARACTER LM\n",
    "def ctc_beam_search(out,labels, prune=0.0001, k=20, lm=None,alpha=0.3,beta=12):\n",
    "    \"implements CTC Prefix Search Decoding Algorithm as shown by Graves\"\n",
    "\n",
    "    bc_i = 0 # blank/special charatcter index \n",
    "    F = out.shape[1]\n",
    "    out = np.vstack((np.zeros(F), out))\n",
    "    steps = out.shape[0]\n",
    "    W = lambda l: re.findall(r'\\w+[\\s|>]', l)\n",
    "    \n",
    "    pb, pnb = defaultdict(Counter), defaultdict(Counter)\n",
    "    pb[0][''], pnb[0][''] = 1, 0\n",
    "    prev_beams = ['']\n",
    "    for t in range(1,steps):\n",
    "        pruned_alphabet = [labels[i] for i in np.where(out[t] > prune)[0]]\n",
    "        for b in prev_beams:\n",
    "            for c_t in pruned_alphabet:\n",
    "                index = labels.index(c_t)\n",
    "                #Collapsing case (copy case as the last character in the beam)\n",
    "                if c_t == \"_\": #Extending with a blank\n",
    "                    pb[t][b] += out[t][index]*(pb[t-1][b] + pnb[t-1][b])   \n",
    "                else:\n",
    "                    i_plus = b + c_t\n",
    "                    if len(b) > 0 and c_t == b[-1]: #Extending with the same character as the last one\n",
    "                        pnb[t][b] += out[t][index]*pnb[t-1][b]\n",
    "                        pnb[t][i_plus] += out[t][index]*pb[t-1][b]\n",
    "                    #expanding the beam (extend case as the last character is different)\n",
    "                    elif c_t == \" \" and len(b.replace(' ', '')) > 0 : # LM constraints\n",
    "                        prob = [i[0] for i in lm.full_scores(i_plus,eos=False,bos=False)][-1]\n",
    "                        lm_p = (10**prob)**alpha\n",
    "                        pnb[t][i_plus] += lm_p*out[t][index]*(pb[t-1][b] + pnb[t-1][b])\n",
    "                    else:\n",
    "                        pnb[t][i_plus] += out[t][index]*(pb[t-1][b] + pnb[t-1][b])\n",
    "\n",
    "        ptot = pb[t] + pnb[t]\n",
    "        for i in ptot.keys():\n",
    "            ptot[i] = ptot[i]*(len(i)+1)**beta\n",
    "        prev_beams = sort_beam(ptot,k)\n",
    "    return prev_beams[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
