{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import json, pickle, os, string, tqdm, kenlm, json\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import groupby\n",
    "import Levenshtein as Lev\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lse(*args): # logsumexp trick\n",
    "    a_max = max(args)\n",
    "    lsp = math.log(sum(math.exp(a-a_max) for a in args))\n",
    "    return a_max + lsp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s1 = True text\n",
    "#s2 = predicted text\n",
    "\n",
    "def wer_(s1, s2):\n",
    "    \"\"\"\n",
    "    Computes the Word Error Rate, defined as the edit distance between the\n",
    "    two provided sentences after tokenizing to words.\n",
    "    Arguments:\n",
    "        s1 (string): space-separated sentence\n",
    "        s2 (string): space-separated sentence\n",
    "    \"\"\"\n",
    "\n",
    "    # build mapping of words to integers\n",
    "    b = set(s1.split() + s2.split())\n",
    "    word2char = dict(zip(b, range(len(b))))\n",
    "\n",
    "    # map the words to a char array (Levenshtein packages only accepts\n",
    "    # strings)\n",
    "    w1 = [chr(word2char[w]) for w in s1.split()]\n",
    "    w2 = [chr(word2char[w]) for w in s2.split()]\n",
    "    \n",
    "    return Lev.distance(''.join(w1), ''.join(w2))\n",
    "\n",
    "def cer_(s1, s2):\n",
    "    \"\"\"\n",
    "    Computes the Character Error Rate, defined as the edit distance.\n",
    "\n",
    "    Arguments:\n",
    "        s1 (string): space-separated sentence\n",
    "        s2 (string): space-separated sentence\n",
    "    \"\"\"\n",
    "    s1, s2, = s1.replace(' ', ''), s2.replace(' ', '')\n",
    "\n",
    "    return Lev.distance(s1, s2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#When using the above implementation, use the code belove to calculate the wer in percentatge: \n",
    "#pred = list of ouput prediction of model (it is the text) # example [\" MY NAME IS HEMANT\", \" I AM A GOD\"]\n",
    "# total_wer = 0\n",
    "# for x in range(len(pred)):\n",
    "#     transcript, reference = data_[x][1], pred[x]\n",
    "#     wer_inst = wer(transcript, reference)\n",
    "#     total_wer += float(wer_inst)\n",
    "# print(\"WER is : \",total_wer/len(pred),\"%\")\n",
    "# wer_(pred,true)/len(pred.split(' '))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = np.load(\"out.npy\")\n",
    "with open(\"true.txt\", \"r\") as f:\n",
    "    reference = f.read()\n",
    "with open(\"pred.txt\", \"r\") as f:\n",
    "    transcript = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_best_path(out,labels):\n",
    "    \"implements best path decoding as shown by Graves\"\n",
    "    out = [labels[i] for i in np.argmax(out, axis=1) if i!=labels[-1]]\n",
    "    o = \"\"\n",
    "    for i,j in groupby(out):\n",
    "        o = o + i\n",
    "    return o.replace(\"_\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = \"_'ABCDEFGHIJKLMNOPQRSTUVWXYZ \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUR PRODATIVITY AND OUR COMMITMENT TO OUR CLIENTS XXXUM TO MAKE SURE THAT WE NEED WHAT BE XXXUM PROMISED FOR THE DEADLINE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19.047619047619047"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gred_txt = ctc_best_path(out,labels)\n",
    "print(gred_txt)\n",
    "wer_(gred_txt,reference)/len(reference.split(' '))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WORD LM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_w = kenlm.LanguageModel('/home/hemant/sopi_deep/lm/3_gram_full.arpa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_beam(ptot,k):\n",
    "    if len(ptot) < k:\n",
    "        return [i for i in ptot.keys()]\n",
    "    else:\n",
    "        dict_ = sorted(dict((v,k) for k,v in ptot.items()).items(),reverse=True)[:k]\n",
    "        return [i[1] for i in dict_]\n",
    "\n",
    "#using WORD LM\n",
    "def ctc_beam_search(out,labels, prune=0.0001, k=20, lm=None,alpha=0.3,beta=12):\n",
    "    \"implements CTC Prefix Search Decoding Algo13.043478260869565%'rithm as shown by Graves\"\n",
    "    '''\n",
    "    out = ctc output\n",
    "    labels = string of labels\n",
    "    prune = prune the ctc output\n",
    "    k=beam-width\n",
    "    lm=word age model used\n",
    "    alpha,beta = hyper-parameters\n",
    "    '''\n",
    "\n",
    "    bc_i = 0 # blank/special charatcter index \n",
    "    F = out.shape[1]\n",
    "    out = np.vstack((np.zeros(F), out))\n",
    "    steps = out.shape[0]\n",
    "    \n",
    "    pb, pnb = defaultdict(Counter), defaultdict(Counter)\n",
    "    pb[0][''], pnb[0][''] = 1, 0\n",
    "    prev_beams = ['']\n",
    "    for t in range(1,steps):\n",
    "        pruned_alphabet = [labels[i] for i in np.where(out[t] > prune)[0]]\n",
    "        for b in prev_beams:\n",
    "            \n",
    "            for c_t in pruned_alphabet:\n",
    "                index = labels.index(c_t)\n",
    "                #Collapsing case (copy case as the last character in the beam)\n",
    "                if c_t == \"_\": #Extending with a blank\n",
    "                    pb[t][b] = out[t][index]*(pb[t-1][b] + pnb[t-1][b])\n",
    "                    continue\n",
    "                else:\n",
    "                    i_plus = b + c_t\n",
    "                    if len(b) > 0 and c_t == b[-1]: #Extending with the same character as the last one\n",
    "                        pnb[t][b] += out[t][index]*pnb[t-1][b]\n",
    "                        pnb[t][i_plus] += out[t][index]*pb[t-1][b]\n",
    "                    #Extending case as the last character is different\n",
    "                    elif c_t == \" \" and len(b.replace(' ', '')) > 0 : # LM constraints\n",
    "                        prob = [i[0] for i in lm.full_scores(i_plus,eos=False,bos=False)][-1]\n",
    "                        lm_p = (10**prob)**alpha\n",
    "                        pnb[t][i_plus] += lm_p*out[t][index]*(pb[t-1][b] + pnb[t-1][b])\n",
    "                    else:\n",
    "                        pnb[t][i_plus] += out[t][index]*(pb[t-1][b] + pnb[t-1][b])\n",
    "                    # If the new beam is not in the previous beams\n",
    "                    if i_plus not in prev_beams:\n",
    "                        pb[t][i_plus] += out[t][labels.index(\"_\")] * (pb[t - 1][i_plus] + pnb[t - 1][i_plus])\n",
    "                        pnb[t][i_plus] += out[t][index] * pnb[t - 1][i_plus]\n",
    "\n",
    "        ptot = pb[t] + pnb[t]\n",
    "        for i in ptot.keys():\n",
    "            ptot[i] = ptot[i]*(len(i.split())+1)**beta\n",
    "        prev_beams = sort_beam(ptot,k)\n",
    "    return prev_beams[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I THINK IT IS GOINGNABEAVORYALDAIDEAINTHEBEASTINTINTELLETIONALDATIBUSINESSESINTHERYEARSTOCOMEIDONTWT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "92.3076923076923"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beam_txt = ctc_beam_search(out,labels,0.0001,k=100,lm=lm_w,alpha=1.2,beta=4)\n",
    "print(beam_txt)\n",
    "wer_(beam_txt,reference)/len(reference.split(' '))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHARACTER LM Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_c = kenlm.LanguageModel('/home/hemant/E2E_NER-Through-Speech/LM/3_char_gram.arpa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_beam(ptot,k):\n",
    "    if len(ptot) < k:\n",
    "        return [i for i in ptot.keys()]\n",
    "    else:\n",
    "        dict_ = sorted(dict((v,k) for k,v in ptot.items()).items(),reverse=True)[:k]\n",
    "        return [i[1] for i in dict_]\n",
    "#using CHARACTER LM\n",
    "def ctc_beam_search_clm(out,labels, prune=0.001, k=20, lm=None,alpha=0.3,beta=12):\n",
    "    \"implements CTC Prefix Search Decoding Algorithm as shown by Graves\"\n",
    "\n",
    "    bc_i = 0 # blank/special charatcter index \n",
    "    F = out.shape[1]\n",
    "    out = np.vstack((np.zeros(F), out))\n",
    "    steps = out.shape[0]\n",
    "    \n",
    "    pb, pnb = defaultdict(Counter), defaultdict(Counter)\n",
    "    pb[0][''], pnb[0][''] = 1, 0\n",
    "    prev_beams = ['']\n",
    "    for t in range(1,steps):\n",
    "        pruned_alphabet = [labels[i] for i in np.where(out[t] > prune)[0]]\n",
    "        for b in prev_beams:\n",
    "            for c_t in pruned_alphabet:\n",
    "                index = labels.index(c_t)\n",
    "                #Collapsing case (copy case as the last character in the beam)\n",
    "                if c_t == \"_\": #Extending with a blank\n",
    "                    pb[t][b] += out[t][index]*(pb[t-1][b] + pnb[t-1][b])  \n",
    "                else:  # LM constraints\n",
    "                    i_plus = b + c_t\n",
    "                     #Extending with the same character as the last one\n",
    "                    if len(b) > 0 and c_t == b[-1]:\n",
    "                        pnb[t][b] += out[t][index]*pnb[t-1][b]\n",
    "                        pnb[t][i_plus] += out[t][index]*pb[t-1][b]\n",
    "                    #expanding the beam (extend case as the last character is different)\n",
    "                    elif len(b.replace(' ', '')) > 0 :\n",
    "                        prob = [i[0] for i in lm.full_scores(i_plus,eos=False,bos=False)][-1]\n",
    "                        lm_p = 1#(10**prob)**alpha\n",
    "                        pnb[t][i_plus] += lm_p*out[t][index]*(pb[t-1][b] + pnb[t-1][b])\n",
    "                    else:\n",
    "                        pnb[t][i_plus] += out[t][index]*(pb[t-1][b] + pnb[t-1][b])\n",
    "        ptot = pb[t] + pnb[t]\n",
    "        for i in ptot.keys():\n",
    "            ptot[i] = ptot[i]*(len(i)+1)**beta\n",
    "        prev_beams = sort_beam(ptot,k)\n",
    "    return prev_beams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPLEMANTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/hemant/E2E_NER-Through-Speech/S2T/\")\n",
    "from opts import add_decoder_args, add_inference_args\n",
    "from utils import load_model\n",
    "import os\n",
    "from ctc_decoders import *\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from data.data_loader import SpectrogramParser\n",
    "\n",
    "from opts import add_decoder_args, add_inference_args\n",
    "from utils import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = \"_'ABCDEFGHIJKLMNOPQRSTUVWXYZ \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 19/50 [00:23<00:37,  1.22s/it]"
     ]
    }
   ],
   "source": [
    "# decoding = 'greedy'\n",
    "decoding = 'beam_w'\n",
    "# decoding = 'beam_c'\n",
    "prune = 0.0001\n",
    "beam_width = 100\n",
    "alpha = 1.2\n",
    "beta = 4\n",
    "lm = lm_w\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "device = torch.device(\"cuda\")\n",
    "model = load_model(device, \"/home/hemant/sopi_deep/models/deep/finetuen_sopi.pth\")\n",
    "spect_parser = SpectrogramParser(model.audio_conf, normalize=True)\n",
    "\n",
    "torch.cuda.set_device(int(0))\n",
    "with open(\"/home/hemant/sopi_deep/data/deepspeech/test/ata/test.csv\",\"r\") as f:\n",
    "    csv = f.readlines()\n",
    "\n",
    "total_cer, total_wer, num_tokens, num_chars = 0, 0, 0, 0\n",
    "lm = lm_w\n",
    "output = []\n",
    "for i in tqdm(csv):\n",
    "    audio_path, reference_path = i.split(\",\")\n",
    "\n",
    "    spect = spect_parser.parse_audio(audio_path).contiguous()\n",
    "    spect = spect.view(1, 1, spect.size(0), spect.size(1))\n",
    "    spect = spect.to(device)\n",
    "\n",
    "    input_sizes = torch.IntTensor([spect.size(3)]).int()\n",
    "    out, output_sizes = model(spect, input_sizes)\n",
    "    out = out.cpu().detach().numpy()[0]\n",
    "\n",
    "    if decoding == \"greedy\": transcript = ctc_best_path(out,labels)\n",
    "    elif decoding == \"beam_w\": transcript = ctc_beam_search(out,labels,prune,beam_width,lm,alpha,beta)\n",
    "    elif decoding == \"beam_c\": transcript = ctc_beam_search_clm(out,labels,prune,beam_width,lm,alpha=alpha,beta=beta)\n",
    "        \n",
    "    with open(reference_path.replace(\"\\n\",\"\"),\"r\") as f:\n",
    "        reference = f.readline()\n",
    "    output.append([transcript,reference])\n",
    "    wer_inst = wer_(transcript,reference)\n",
    "    cer_inst = cer_(transcript, reference)\n",
    "    total_wer += wer_inst\n",
    "    total_cer += cer_inst\n",
    "    num_tokens += len(reference.split(' '))\n",
    "    num_chars += len(reference.replace(' ', ''))\n",
    "        \n",
    "wer = (float(total_wer) / num_tokens)*100\n",
    "cer = (float(total_cer) / num_chars)*100\n",
    "print('Test Summary \\t'\n",
    "    'Average WER {wer:.3f}\\t'\n",
    "    'Average CER {cer:.3f}\\t'.format(wer=wer, cer=cer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Summary \tAverage WER 67.256\tAverage CER 23.264"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Summary \tAverage WER 48.410\tAverage CER 24.264\t greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path,reference_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = audio_path + \",\" +reference_path\n",
    "with open(\"/home/hemant/junk/out.csv\",\"w\") as f:\n",
    "    f.write(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_(transcript,reference)/len(reference.split(' '))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "beam_txt=ctc_beam_search(out,labels,0.0001,k=100,lm=lm_w,alpha=1.2,beta=6)\n",
    "print(beam_txt)\n",
    "wer_(beam_txt,reference)/len(reference.split(' '))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_txt=ctc_beam_search(out,labels,0.001,k=100,lm=lm_w,alpha=0,beta=0)\n",
    "print(beam_txt)\n",
    "wer_(beam_txt,reference)/len(reference.split(' '))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_txt=ctc_beam_search_clm(out,labels,0,k=10, lm_c = lm_c, alpha=0.1,beta=6)\n",
    "print(beam_txt)\n",
    "wer_(beam_txt,reference)/len(reference.split(' '))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    output.append([transcript,reference])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import collections\n",
    "\n",
    "NEG_INF = -float(\"inf\")\n",
    "\n",
    "def make_new_beam():\n",
    "  fn = lambda : (NEG_INF, NEG_INF)\n",
    "  return collections.defaultdict(fn)\n",
    "\n",
    "def logsumexp(*args):\n",
    "  \"\"\"\n",
    "  Stable log sum exp.\n",
    "  \"\"\"\n",
    "  if all(a == NEG_INF for a in args):\n",
    "      return NEG_INF\n",
    "  a_max = max(args)\n",
    "  lsp = math.log(sum(math.exp(a - a_max)\n",
    "                      for a in args))\n",
    "  return a_max + lsp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_beam(ptot,k):\n",
    "    if len(ptot) < k:\n",
    "        return [i for i in ptot.keys()]\n",
    "    else:\n",
    "        dict_ = sorted(dict((v,k) for k,v in ptot.items()).items(),reverse=True)[:k]\n",
    "        return [i[1] for i in dict_]\n",
    "\n",
    "#using WORD LM\n",
    "def ctc_beam_search(out,labels, prune=0.0001, k=20, lm=None,alpha=0.3,beta=12):\n",
    "    \"implements CTC Prefix Search Decoding Algo13.043478260869565%'rithm as shown by Graves\"\n",
    "\n",
    "    bc_i = 0 # blank/special charatcter index \n",
    "    F = out.shape[1]\n",
    "    out = np.vstack((np.zeros(F), out))\n",
    "    steps = out.shape[0]\n",
    "    \n",
    "    pb, pnb = defaultdict(Counter), defaultdict(Counter)\n",
    "    pb[0][''], pnb[0][''] = 1, 0\n",
    "    prev_beams = ['']\n",
    "    for t in range(1,steps):\n",
    "        pruned_alphabet = [labels[i] for i in np.where(out[t] > prune)[0]]\n",
    "        for b in prev_beams:\n",
    "            for c_t in pruned_alphabet:\n",
    "                index = labels.index(c_t)\n",
    "                #Collapsing case (copy case as the last character in the beam)\n",
    "                if c_t == \"_\": #Extending with a blank\n",
    "                    pb[t][b] += out[t][index]*(pb[t-1][b] + pnb[t-1][b])   \n",
    "                else:\n",
    "                    i_plus = b + c_t\n",
    "                    if len(b) > 0 and c_t == b[-1]: #Extending with the same character as the last one\n",
    "                        pnb[t][b] += out[t][index]*pnb[t-1][b]\n",
    "                        pnb[t][i_plus] += out[t][index]*pb[t-1][b]\n",
    "                    #expanding the beam (extend case as the last character is different)\n",
    "                    elif c_t == \" \" and len(b.replace(' ', '')) > 0 : # LM constraints\n",
    "                        prob = [i[0] for i in lm.full_scores(i_plus,eos=False,bos=False)][-1]\n",
    "                        lm_p = (10**prob)**alpha\n",
    "                        pnb[t][i_plus] += lm_p*out[t][index]*(pb[t-1][b] + pnb[t-1][b])\n",
    "                    else:\n",
    "                        pnb[t][i_plus] += out[t][index]*(pb[t-1][b] + pnb[t-1][b])\n",
    "                        \n",
    "                    if i_plus not in prev_beams:\n",
    "                        pb[t][i_plus] += out[t][index] * (pb[t - 1][i_plus] + pnb[t - 1][i_plus])\n",
    "                        pnb[t][i_plus] += out[t][index] * pnb[t - 1][i_plus]\n",
    "\n",
    "\n",
    "        ptot = pb[t] + pnb[t]\n",
    "        for i in ptot.keys():\n",
    "            ptot[i] = ptot[i]*(len(i)+1)**beta\n",
    "        prev_beams = sort_beam(ptot,k)\n",
    "    return prev_beams[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_txt=ctc_beam_search(out,labels,0,k=1,lm=lm_w,alpha=0,beta=0)\n",
    "print(beam_txt)\n",
    "wer_(beam_txt,reference)/len(reference.split(' '))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def decode(probs, beam_size=100, blank=0):\n",
    "\n",
    "  T, S = probs.shape\n",
    "  probs = np.log(probs)\n",
    "\n",
    "  beam = [(tuple(), (0.0, NEG_INF))]\n",
    "\n",
    "  for t in range(T): # Loop over time\n",
    "    next_beam = make_new_beam()\n",
    "\n",
    "    for s in range(S): # Loop over vocab\n",
    "      p = probs[t, s]\n",
    "      for prefix, (p_b, p_nb) in beam: # Loop over beam\n",
    "        if s == blank:\n",
    "            n_p_b, n_p_nb = next_beam[prefix]\n",
    "            n_p_b = logsumexp(n_p_b, p_b + p, p_nb + p)\n",
    "            next_beam[prefix] = (n_p_b, n_p_nb)\n",
    "            continue\n",
    "        end_t = prefix[-1] if prefix else None\n",
    "        n_prefix = prefix + (s,)\n",
    "        n_p_b, n_p_nb = next_beam[n_prefix]\n",
    "        if s != end_t:\n",
    "          n_p_nb = logsumexp(n_p_nb, p_b + p, p_nb + p)\n",
    "        else:\n",
    "          n_p_nb = logsumexp(n_p_nb, p_b + p)\n",
    "          \n",
    "        next_beam[n_prefix] = (n_p_b, n_p_nb)\n",
    "        if s == end_t:\n",
    "          n_p_b, n_p_nb = next_beam[prefix]\n",
    "          n_p_nb = logsumexp(n_p_nb, p_nb + p)\n",
    "          next_beam[prefix] = (n_p_b, n_p_nb)\n",
    "    beam = sorted(next_beam.items(),\n",
    "            key=lambda x : logsumexp(*x[1]),\n",
    "            reverse=True)\n",
    "    beam = beam[:beam_size]\n",
    "\n",
    "  best = beam[0]\n",
    "  return best[0], -logsumexp(*best[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "time = 50\n",
    "output_dim = 20\n",
    "\n",
    "probs = out # np.random.rand(time, output_dim)\n",
    "probs = probs / np.sum(probs, axis=1, keepdims=True)\n",
    "\n",
    "label, score = decode(probs)\n",
    "\n",
    "print(\"Score {:.3f}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_ = ''\n",
    "for i in label:\n",
    "    out_+=labels[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_(out_,reference)/len(reference.split(' '))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# decoding = 'greedy'\n",
    "decoding = 'beam_w'\n",
    "# decoding = 'beam_c'\n",
    "prune = 0\n",
    "beam_width = 100\n",
    "alpha = 0\n",
    "beta = 0\n",
    "lm = lm_w\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "device = torch.device(\"cuda\")\n",
    "model = load_model(device, \"/home/hemant/sopi_deep/models/deep/finetuen_sopi.pth\")\n",
    "spect_parser = SpectrogramParser(model.audio_conf, normalize=True)\n",
    "\n",
    "torch.cuda.set_device(int(0))\n",
    "with open(\"/home/hemant/sopi_deep/data/deepspeech/test/ata/test.csv\",\"r\") as f:\n",
    "    csv = f.readlines()\n",
    "\n",
    "total_cer, total_wer, num_tokens, num_chars = 0, 0, 0, 0\n",
    "lm = lm_w\n",
    "output = []\n",
    "for i in tqdm(csv):\n",
    "    audio_path, reference_path = i.split(\",\")\n",
    "\n",
    "    spect = spect_parser.parse_audio(audio_path).contiguous()\n",
    "    spect = spect.view(1, 1, spect.size(0), spect.size(1))\n",
    "    spect = spect.to(device)\n",
    "\n",
    "    input_sizes = torch.IntTensor([spect.size(3)]).int()\n",
    "    out, output_sizes = model(spect, input_sizes)\n",
    "    out = out.cpu().detach().numpy()[0]\n",
    "\n",
    "    label, score = decode(out)\n",
    "    transcript_ = ''\n",
    "    for i in label:\n",
    "        transcript_+=labels[i]\n",
    "    \n",
    "    with open(reference_path.replace(\"\\n\",\"\"),\"r\") as f:\n",
    "        reference = f.readline()\n",
    "        \n",
    "    print(transcript_)\n",
    "    wer_inst = wer_(transcript_,reference)\n",
    "    cer_inst = cer_(transcript_, reference)\n",
    "    total_wer += wer_inst\n",
    "    total_cer += cer_inst\n",
    "    num_tokens += len(reference.split(' '))\n",
    "    num_chars += len(reference.replace(' ', ''))\n",
    "        \n",
    "wer = (float(total_wer) / num_tokens)*100\n",
    "cer = (float(total_cer) / num_chars)*100\n",
    "print('Test Summary \\t'\n",
    "    'Average WER {wer:.3f}\\t'\n",
    "    'Average CER {cer:.3f}\\t'.format(wer=wer, cer=cer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = np.array([[0.4, 0, 0.6], [0.4, 0, 0.6]])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
